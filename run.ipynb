{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target_column'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mu:\\ML_OPS\\deploying-machine-learning-models\\00_productionMLCode\\run.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/ML_OPS/deploying-machine-learning-models/00_productionMLCode/run.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mprediction_model/datasets/train.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/ML_OPS/deploying-machine-learning-models/00_productionMLCode/run.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Example preprocessing and feature engineering steps\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/ML_OPS/deploying-machine-learning-models/00_productionMLCode/run.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mtarget_column\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Replace 'target_column' with your target variable\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/ML_OPS/deploying-machine-learning-models/00_productionMLCode/run.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtarget_column\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/ML_OPS/deploying-machine-learning-models/00_productionMLCode/run.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mu:\\ML_OPS\\deploying-machine-learning-models\\00_productionMLCode\\venv_pack\\lib\\site-packages\\pandas\\core\\frame.py:3990\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   3859\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   3860\u001b[0m     labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3866\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3867\u001b[0m ):\n\u001b[0;32m   3868\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3869\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   3870\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3988\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   3989\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3990\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   3991\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   3992\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   3993\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   3994\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3995\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   3996\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   3997\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3998\u001b[0m     )\n",
      "File \u001b[1;32mu:\\ML_OPS\\deploying-machine-learning-models\\00_productionMLCode\\venv_pack\\lib\\site-packages\\pandas\\core\\generic.py:3936\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   3935\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 3936\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   3938\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   3939\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mu:\\ML_OPS\\deploying-machine-learning-models\\00_productionMLCode\\venv_pack\\lib\\site-packages\\pandas\\core\\generic.py:3970\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   3969\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3970\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   3971\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{axis_name: new_axis})\n\u001b[0;32m   3973\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   3974\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mu:\\ML_OPS\\deploying-machine-learning-models\\00_productionMLCode\\venv_pack\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5018\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   5017\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 5018\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5019\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   5020\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target_column'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "data = pd.read_csv('prediction_model/datasets/train.csv')\n",
    "\n",
    "# Example preprocessing and feature engineering steps\n",
    "X = data.drop('L', axis=1)  # Replace 'target_column' with your target variable\n",
    "y = data['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "model_filename = 'classification_v2.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprt libraries\n",
    "import pathlib \n",
    "import os\n",
    "import prediction_model \n",
    "\n",
    "PACKAGE_ROOT = pathlib.Path(prediction_model.__file__).resolve().parent\n",
    "\n",
    "DATAPATH = os.path.join(PACKAGE_ROOT,'datasets')\n",
    "SAVED_MODEL_PATH = os.path.join(PACKAGE_ROOT,'trained_models')\n",
    "\n",
    "TRAIN_FILE = 'train.csv'\n",
    "TEST_FILE = 'test.csv'\n",
    "\n",
    "TARGET = 'Loan_Status'\n",
    "\n",
    "#Features to keep\n",
    "FEATURES=['Gender','Married','Dependents',\n",
    "    'Education','Self_Employed','ApplicantIncome',\n",
    "    'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n",
    "    'Credit_History','Property_Area'] # Final feature to keep in data\n",
    "\n",
    "NUMERICAL_FEATURES=['ApplicantIncome', 'LoanAmount', 'Loan_Amount_Term'] \n",
    "\n",
    "CATEGORICAL_FEATURES=['Gender','Married','Dependents',\n",
    "'Education','Self_Employed','Credit_History','Property_Area'] #Categorical\n",
    "\n",
    "FEATURES_TO_ENCODE=['Gender','Married','Dependents',\n",
    "'Education','Self_Employed','Credit_History','Property_Area'] #Features to Encode\n",
    "\n",
    "TEMPORAL_FEATURES=['ApplicantIncome']\n",
    "TEMPORAL_ADDITION='CoapplicantIncome'\n",
    "LOG_FEATURES=['ApplicantIncome', 'LoanAmount'] #Features for Log Transformation\n",
    "DROP_FEATURES=['CoapplicantIncome'] #Features to Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os \n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Import other files/modules \n",
    "from prediction_model.config import config\n",
    "# Imports all the LOCAL and GLOBAL paths and variables \n",
    "\n",
    "def load_dataset(file_name):\n",
    "    '''Read data'''\n",
    "    file_path = os.path.join(config.DATAPATH,file_name) # DATAPATH is dataset dir \n",
    "    _data = pd.read_csv(file_path)\n",
    "    return _data \n",
    "\n",
    "def save_pipeline(pipeline_to_save):\n",
    "    \"\"\" Store output of pipeline \n",
    "        Exporting pickle file of trained model\n",
    "    \"\"\"\n",
    "    save_file_name = 'classification_v1.pkl'\n",
    "    save_path = os.path.join(config.SAVED_MODEL_PATH,save_file_name)\n",
    "    joblib.dump(pipeline_to_save,save_path)\n",
    "    print(\"Saved pipeline :\",save_file_name)\n",
    "    \n",
    "def load_pipeline(pipeline_to_load):\n",
    "    '''Importing the pickle file'''\n",
    "    save_path = os.path.join(config.SAVED_MODEL_PATH,pipeline_to_load)\n",
    "    trained_model = joblib.load(save_path)\n",
    "    return trained_model\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import other files/modules \n",
    "from prediction_model.config import config\n",
    "\n",
    "# Numeric Imputer\n",
    "class NumericalImputer(BaseEstimator,TransformerMixin):\n",
    "    \"Numerical Data Missing Value Imputer\"\n",
    "    def __init__(self,variables=None):\n",
    "        self.variables = variables\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.imputer_dict = {}\n",
    "        for feature in self.variables:\n",
    "            self.imputer_dict[feature] = X[feature].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature].fillna(self.imputer_dict[feature],inplace=True)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "#Categorical Imputer\n",
    "class CategoricalImputer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Categorical Data Missing Value Imputer\"\"\"\n",
    "    def __init__(self, variables=None):\n",
    "        self.variables = variables\n",
    "    \n",
    "    def fit(self, X,y=None):\n",
    "        self.imputer_dict_={}\n",
    "        for feature in self.variables:\n",
    "            self.imputer_dict_[feature] = X[feature].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X=X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature].fillna(self.imputer_dict_[feature],inplace=True)\n",
    "        return X\n",
    "    \n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Categorical Data Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, variables=None):\n",
    "        self.variables = variables\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.encoder_dict_ = {}\n",
    "        for var in self.variables:\n",
    "            t = X[var].value_counts().sort_values(ascending=True).index\n",
    "            self.encoder_dict_[var] = {k: i for i, k in enumerate(t, 0)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.encoder_dict_[feature])\n",
    "        return X\n",
    "\n",
    "\n",
    "class TemporalVariableEstimator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature Engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, variables=None, reference_variable=None):\n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No need to put anything, needed for Sklearn Pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.variables:\n",
    "            X[var] = X[var] + X[self.reference_variable]\n",
    "        return X\n",
    "\n",
    "\n",
    "class LogTransformation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforming variables using Log Transformations\"\"\"\n",
    "    \n",
    "    def __init__(self, variables=None):\n",
    "        self.variables = variables\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    #Need to check in advance if the features are <= 0\n",
    "    #If yes, needs to be transformed properly (E.g., np.log1p(X[var]))\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for var in self.variables:\n",
    "            X[var] = np.log(X[var])\n",
    "        return X\n",
    "\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Dropping Features Which Are Less Significant\"\"\"\n",
    "    \n",
    "    def __init__(self, variables_to_drop=None):\n",
    "        self.variables_to_drop = variables_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X = X.drop(self.variables_to_drop, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import other files/modules\n",
    "from prediction_model.config import config\n",
    "import prediction_model.processing.preprocessors as pp\n",
    "\n",
    "loan_pipe = Pipeline([\n",
    "    ('Numerical Imputer', pp.NumericalImputer(variables=config.NUMERICAL_FEATURES)),\n",
    "    ('Categorical Imputer', pp.CategoricalImputer(variables=config.CATEGORICAL_FEATURES)),\n",
    "    ('Temporal Features', pp.TemporalVariableEstimator(variables=config.TEMPORAL_FEATURES, reference_variable=config.TEMPORAL_ADDITION)),\n",
    "    ('Categorical Encoder', pp.CategoricalEncoder(variables=config.FEATURES_TO_ENCODE)),\n",
    "    ('Log Transform', pp.LogTransformation(variables=config.LOG_FEATURES)),\n",
    "    ('Drop Features', pp.DropFeatures(variables_to_drop=config.DROP_FEATURES)),\n",
    "    ('Scaler Transform', MinMaxScaler()),\n",
    "    ('Linear Model', LogisticRegression(random_state=1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Import other files/modules\n",
    "from prediction_model.config import config\n",
    "from prediction_model.processing.data_management import load_pipeline\n",
    "\n",
    "pipeline_file_name = 'classification_v1.pkl'\n",
    "\n",
    "_loan_pipe = load_pipeline(pipeline_file_name)\n",
    "\n",
    "def _make_prediction(input_data):\n",
    "    '''Predict the output'''\n",
    "    \n",
    "    #Read Data \n",
    "    data = pd.DataFrame(input_data)\n",
    "    \n",
    "    # Prediction \n",
    "    prediction = _loan_pipe.predict(data[config.FEATURES])\n",
    "    output = np.where(prediction==1,'Y','N').tolist()\n",
    "    results = {'prediction':output}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline : classification_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Import other files and modules\n",
    "from prediction_model.config import config\n",
    "from prediction_model.processing.data_management import load_dataset, save_pipeline\n",
    "import prediction_model.processing.preprocessors as pp\n",
    "import prediction_model.pipeline as pl\n",
    "# from prediction_model.predict import _make_prediction\n",
    "\n",
    "def run_training():\n",
    "    '''Train the model'''\n",
    "    train = load_dataset(config.TRAIN_FILE)\n",
    "    \n",
    "    # Separating Loan_Status in y\n",
    "    y = train[config.TARGET].map({'N':0,'Y':1})\n",
    "    pl.loan_pipe.fit(train[config.FEATURES],y)\n",
    "    save_pipeline(pipeline_to_save=pl.loan_pipe)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    run_training()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline : classification_v1.pkl\n",
      "{'prediction': ['Y']}\n"
     ]
    }
   ],
   "source": [
    "import prediction_model\n",
    "from prediction_model import train_pipeline\n",
    "from prediction_model.predict import make_prediction\n",
    "import pandas as pd\n",
    "\n",
    "train_pipeline.run_training() # Save the pickle object of the trained model\n",
    "\n",
    "test_data = pd.read_csv(\"prediction_model/datasets/test.csv\") # Load the data\n",
    "result = make_prediction(test_data[0:1]) # Make prediction on the first row\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
